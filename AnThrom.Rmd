---
output:
  html_notebook:
    number_sections: no
    theme: default
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
editor_options: 
  chunk_output_type: inline
---
#**AnThroM**
**testing the relation between Theory-of-Mind network activation and dispositional anthropomorphism**  
by *Ruud Hortensius and Michaela Kent (University of Glasgow) - June 2019 - ...*

# 1. Details {.tabset}

## 1.1 Data 

Data of the Theory-of-Mind functional localiser and Individual Differences in Anthropomorphism Questionnaire are from five different studies.
  
 Dataset_1: Bangor Imaging Unit; EMBOTS; *n*=29 (including 1 pilot scan); full dataset and publication: [Cross...Hortensius (2019)   PTRB](https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0034).  
 
 Dataset_2: Centre for Cognitive NeuroImaging; SHAREDBOTS; *n*=35 (including 2 pilot scans) publication: Hortensius & Cross, in preparation.  
 
 Dataset_3: Centre for Cognitive NeuroImaging; Two studies with the same parameters: *n*=22 (including 2 pilot scans). Social_Gradient_1; *n*=10 (pilot experiment) and BOLDlight; *n*=12.  
 
 Dataset_4: Centre for Cognitive NeuroImaging; GAMEBOTS; *n*=22.  

```{r}
library("tidyverse")

#load own data
DF.dataset1 <- read_tsv(file = "/Volumes/Project0255/dataset_1/participants.tsv")
DF.dataset2 <- read_tsv(file = "/Volumes/Project0255/dataset_2/participants.tsv")
DF.dataset3 <- read_tsv(file = "/Volumes/Project0255/dataset_3/participants.tsv")
DF.dataset4 <- read_tsv(file = "/Volumes/Project0255/dataset_4/participants.tsv")

#combine data
bind_rows(DF.dataset1, DF.dataset2, DF.dataset3,  DF.dataset4, .id = "dataset") %>%
  group_by(dataset) %>% 
  summarise(mean = mean(age), 
            sd = sd(age))
bind_rows(DF.dataset1, DF.dataset2, DF.dataset3,  DF.dataset4, .id = "dataset") %>%
  group_by(dataset, sex) %>% 
  tally()
```


## 1.2 Neuroimaging procedure
 
 All participants completed a Theory-of-Mind localiser ([Jacoby et al., 2016](https://www.sciencedirect.com/science/article/pii/S1053811915010472); [Richardson et al. 2018](https://www.nature.com/articles/s41467-018-03399-2)) and an anatomical scan either in the same session or in two seperate sessions. During the localiser participants passively viewed a short 5.6 min animated film ([Partly Cloudy](https://www.pixar.com/partly-cloudy#partly-cloudy-1)). This movie includes scenes depicting pain (e.g. an alligator biting the main character) and events that trigger mentalizing (e.g. the main character revealing its intention). For dataset_3 and dataset_4 a fieldmap was collected as well. At the end of each experiment participants completed the Individual Differences in Anthropomorphism Questionnaire (IDAQ) ([Waytz et al., 2010](https://journals.sagepub.com/doi/full/10.1177/1745691610369336)). 
 
- BOLD:   
 Dataset_1: 3x3x3.5 voxels, 32 slices, repetition time = 2s, echo time = 30ms  
 Dataset_2: 3mm isotropic, 37 slices, TR = 2s, TE = 30ms  
 Dataset_3: 2mm isotropic, 68 slices, TR = 2s, TE = 27ms  
 Dataset_4:  2.75 x 2.75 x 4, 32 slices, TR = 2s, TE = 13 and 31ms  
 
- T1W:   
 Dataset_1: 1mm isotropic resolution, TR = 12ms, TE = 3.5ms  
 Dataset_2 - 4: 1mm isotropic resolution, TR = 2.3s, TE = 29.6ms (ADNI)  
 
- Fieldmaps:   
 Dataset_1: no, so --use-syn-sdc  
 Dataset_2: no, so --use-syn-sdc  
 Dataset_3: yes  
 Dataset_4: yes  
 
## 1.3 To do
- remove Dicoms after BIDS dataset creation  
- fROI  
- run formal analyses   

# 2. BIDS dataset {.tabset}

## 2.1 Creating the BIDS dataset
For this you need HeuDiConv [Heuristic DICOM Converter](https://github.com/nipy/heudiconv).  
Based on the tutorial by [Franklin Feingold](http://reproducibility.stanford.edu/bids-tutorial-series-part-2a/).

Dowload the latest version of Heudiconv (we used 0.6.0.dev1)

```{bash}
docker pull nipy/heudiconv:latest
```

If on the GRID do
```{bash}
singularity pull docker://nipy/heudiconv:latest
```

Create the info file (dataset_2 - 4)
```{bash}
docker run --rm -it -v /Volumes/Project0255/:/base nipy/heudiconv:latest -d /base/dataset_3/sourcedata/sub-{subject}/*.IMA -o /base/dataset_3 -f convertall -s 315 -c none --overwrite
```

For dataset_1 we first need to convert the .dcm from jpeg-2000 lossless to uncompressed dcm (thanks to Michele Svanera for the code):

```{pyton}
python3 convert_all_compressed_dicom.py
```

Create the info file (dataset_1)
```{bash}
docker run --rm -it -v /Volumes/Project0255/:/base nipy/heudiconv:latest -d /base/dataset_1/sourcedata/sub-{subject}/ses-{session}/*.dcm -o /base/dataset_1 -f convertall -s 129 -ss 01 -c none --overwrite
```

Get the info file
```{bash}
cp /Volumes/Project0255/code/.heudiconv/301/info/dicominfo.tsv /Volumes/Project0255/code
```

## 2.2 Create the heuristic file
Create the following python file and save it in the code folder (don't run the code). There is one functional task (func_movie) and one anatomical (t1w). Dataset_3 and 4 have a field map as well (fmap_phase and fmap_magnitude) Create a heuristic to automatically convert the files.

```{python}
import os
def create_key(template, outtype=('nii.gz',), annotation_classes=None):
    if template is None or not template:
        raise ValueError('Template must be a valid format string')
    return template, outtype, annotation_classes
def infotodict(seqinfo):
    """Heuristic evaluator for determining which runs belong where
    allowed template fields - follow python string module:
    item: index within category
    subject: participant id
    seqitem: run number during scanning
    subindex: sub index within group
    session: session id (only for dataset_1)
    """
    
    t1w1 = create_key('sub-{subject}/{session}/anat/sub-{subject}_{session}_T1w')
    func_movie1 = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-movie_bold')

    t1w = create_key('sub-{subject}/anat/sub-{subject}_T1w')
    func_movie = create_key('sub-{subject}/func/sub-{subject}_task-movie_bold')
    func_movie_echo_1 = create_key('sub-{subject}/func/sub-{subject}_task-movie_echo-1_bold')
    func_movie_echo_2 = create_key('sub-{subject}/func/sub-{subject}_task-movie_echo-2_bold')
    fmap_phase = create_key('sub-{subject}/fmap/sub-{subject}_phasediff')
    fmap_magnitude = create_key('sub-{subject}/fmap/sub-{subject}_magnitude')
    
    info = {t1w1: [], func_movie1: [], t1w: [], func_movie: [], fmap_phase: [], fmap_magnitude: [],
            func_movie_echo_1: [], func_movie_echo_2: []} 
    
    for idx, s in enumerate(seqinfo):
        if ('T1W_1mm_sag SENSE' in s.protocol_name):
            info[t1w1].append(s.series_id)
        if ('ToM_PartlyCloudy SENSE' in s.protocol_name):
            info[func_movie1].append(s.series_id)
        if ('t1_mpr_ns_sag_iso_ADNI_32ch' in s.protocol_name):
            info[t1w].append(s.series_id)
        if ('t1_mpr_ns_sag_P2_ADNI_32ch' in s.protocol_name):
            info[t1w].append(s.series_id)
        if (s.dim4 == 175) and ('FMRI_MB2_p2_2MMISO_TR2_movie' in s.protocol_name):
            info[func_movie].append(s.series_id)
        if (s.dim4 == 175) and ('FMRI_MB2_movie_p2_2MMISO_TR2' in s.protocol_name):
            info[func_movie].append(s.series_id)
        if (s.dim4 == 170) and ('ep2d_ToM_Loc' in s.protocol_name):
            info[func_movie].append(s.series_id)
        if (s.dim4 == 175) and ('ep2d_ToM_Loc' in s.protocol_name):
            info[func_movie].append(s.series_id)
        if (s.dim4 == 175) and ('ep2d_ToM_Loc_boldTR2' in s.protocol_name):
            info[func_movie].append(s.series_id)
        if (s.TE == 13) and ('BP_ep2d_multiecho_32ch_p3_TOM' in s.protocol_name):
            info[func_movie_echo_1].append(s.series_id)
        if (s.TE == 31.36) and ('BP_ep2d_multiecho_32ch_p3_TOM' in s.protocol_name):
            info[func_movie_echo_2].append(s.series_id)
        if (s.dim3 == 92) and ('gre_field_mapping_AAH' in s.protocol_name):
            info[fmap_magnitude].append(s.series_id)
        if (s.dim3 == 46) and ('gre_field_mapping_AAH' in s.protocol_name):
            info[fmap_phase].append(s.series_id)
        if (s.dim3 == 64) and ('gre_field_mapping_AAH' in s.protocol_name):
            info[fmap_magnitude].append(s.series_id)
        if (s.dim3 == 32) and ('gre_field_mapping_AAH' in s.protocol_name):
            info[fmap_phase].append(s.series_id)
    return info
```

Use the heuristic file to convert the Dicom files to .nii.gz (nifti) and create .json files.

```{bash}
docker run --rm -it -v /Volumes/Project0255/:/base nipy/heudiconv:latest -d /base/dataset_4/sourcedata/sub-{subject}/*.IMA -o /base/dataset_4 -f /base/code/heuristic_anthrom.py -s 401 -c dcm2niix -b --overwrite
```

For dataset_1 (for dataset_1 add ses-{session}/ and --ss 01 and .dcm). Movie for sub-101 and 102 is in ses-02. 
```{bash}
docker run --rm -it -v /Volumes/Project0255/:/base nipy/heudiconv:latest -d /base/dataset_1/sourcedata/sub-{subject}/ses-{session}/*.dcm -o /base/dataset_1 -f /base/code/heuristic_anthrom.py -s 121 -ss 02 -c dcm2niix -b --overwrite
```

On the grid do: (sub-116: manual in dcm2niigui)
type in bash before running

Dataset_1
```{bash}
singularity run -B /analyse/Project0255/:/base /analyse/Project0255/my_images/heudiconv_latest.sif -d /base/dataset_1/sourcedata/sub-{subject}/ses-{session}/*.dcm -o /base/dataset_1/ -f /base/code/heuristic_anthrom.py -s 116 -ss 01 -c dcm2niix -b --overwrite
```

Dataset_2 - 4
```{bash}
singularity run -B /analyse/Project0255/:/base /analyse/Project0255/my_images/heudiconv_latest.sif -d /base/dataset_2/sourcedata/sub-{subject}/*.IMA -o /base/dataset_2/ -f /base/code/heuristic_anthrom.py -s 201 -c dcm2niix -b --overwrite
```

## 2.3 Anonymize the data 
Deface using [Pydeface](https://github.com/poldracklab/pydeface). 

```{bash}
#!/bin/bash

set -e 
####For loop that defaces the MRI per subject and replaces the old MRI with the new defaced MRI
rootfolder=/Volumes/Project0255/dataset_4

for subj in 401; do
	echo "Defacing participant $subj"
pydeface ${rootfolder}/sub-${subj}/anat/sub-${subj}_T1w.nii.gz
rm -f ${rootfolder}/sub-${subj}/anat/sub-${subj}_T1w.nii.gz
mv ${rootfolder}/sub-${subj}/anat/sub-${subj}_T1w_defaced.nii.gz ${rootfolder}/sub-${subj}/anat/sub-${subj}_T1w.nii.gz 
done
```


For dataset_1
ses-01: 101 102 103 107 112 113 117 118 119 122 123 124 128
ses-02: 104 105 106 108 109 110 111 115 116 120 121 125 126 127
```{bash}
#!/bin/bash

set -e 
rootfolder=/Volumes/Project0255/dataset_1

for subj in 129; do
	echo "Defacing participant $subj"
for session in 01; do
for echo in 1 2 3 4 5; do
pydeface ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_echo-${echo}_T1w.nii.gz
rm -f ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_echo-${echo}_T1w.nii.gz 
mv ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_echo-${echo}_T1w_defaced.nii.gz ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_echo-${echo}_T1w.nii.gz 
done
done
done
```

## 2.4 Update the .json file for the fmaps for dataset_3 and dataset_4

You need to specify “IntendedFor” field in the _phasediff.json files to point which scans the estimated fieldmap should be applied to.

Run the following script (thanks to Michele Svanera for the code):
```{python}
python change_json.py
```

## 2.5 Combine the dual-echo runs for dataset_4

For dataset_4 we need to combine the two echo's (see [NeuroStar](https://neurostars.org/t/fmriprep-does-not-combine-multi-echo-timeseries/3398/2) for more info. We created a dual_sum volume by adding the two images together (see [Halai et al. 2014](https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.22463). 

Run the following script (thanks to Tyler Morgan for the code):
```{python}
python sum_echo.py
```

## 2.6 BIDS validation

Use the BIDS-Validator to check if the dataset is BIDS compliant.

```{bash}
docker run -ti --rm -v /Volumes/Project0255/dataset_4:/data:ro bids/validator /data
```

# 3. Preprocessing {.tabset}

## 3.1 Run MRQIC
MRIQC is a docker tool to do quality control of the data. More info [here](https://poldracklab.github.io/mriqc/).

MRIQC 0.14.2 was used:

```{bash}
docker run -it --rm -v /Volumes/Project0255/dataset_1/:/data:ro -v /Volumes/Project0255/dataset_1/derivatives/mriqc:/out poldracklab/mriqc:0.14.2 /data /out participant --participant-label 101 -m T1w bold --ica --fft-spikes-detector 
```

On the grid do (cd in /analyse folder):

```{bash}
singularity run --cleanenv /analyse/Project0255/my_images/mriqc-0.14.2.simg /analyse/Project0255/dataset_1 /analyse/Project0255/dataset_1/derivatives/mriqc participant --participant-label 123 -m T1w bold --ica --fft-spikes-detector -w /analyse/Project0255/work
```

Run it seperately for the datasets. Change participant to group to create the group reports.

```{bash}
docker run -it --rm -v /Volumes/Project0255/dataset_4/:/data:ro -v /Volumes/Project0255/dataset_4/derivatives/mriqc:/out poldracklab/mriqc:0.14.2 /data /out group
```

## 3.2 Compare MRIQC

Plot the output. This is based on [MRIQCeption](https://github.com/elizabethbeard/mriqception). The MRIQCeption Visualization by Catherine Walsh was adapted. Adjust the filter if you want to look at different measures.

Adjust this to your liking (e.g. bold: fd_mean, fd_perc, dvars_std, dvars_vstd, gcor, tsnr, t1w: cjv, cnr, snr, efc, inu, wm2max, fwhm) and modality (bold or t1w)
```{r}
QCmeasure <- "fd_mean" 
modality <- "bold"
```

Run the following [Change the script below to load the group results for the different datasets]
```{r}
#libraries
library("tidyverse")
source("/Volumes/Project0255/code/R_rainclouds.R")

#load own data
DF.dataset1  <- read_tsv(file = paste("/Volumes/Project0255/dataset_1/derivatives/mriqc/group_", modality, ".tsv", sep ="")) %>%
  gather("measure", "value", 2:46) %>%
  select("bids_name","measure", "value")

DF.dataset2 <- read_tsv(file = paste("/Volumes/Project0255/dataset_2/derivatives/mriqc/group_", modality, ".tsv", sep ="")) %>%
  gather("measure", "value", 2:46) %>%
  select("bids_name","measure", "value")

DF.dataset3 <- read_tsv(file = paste("/Volumes/Project0255/dataset_3/derivatives/mriqc/group_", modality, ".tsv", sep ="")) %>%
  gather("measure", "value", 2:46) %>%
  select("bids_name","measure", "value")

DF.dataset4 <- read_tsv(file = paste("/Volumes/Project0255/dataset_4/derivatives/mriqc/group_", modality, ".tsv", sep ="")) %>%
  gather("measure", "value", 2:46) %>%
  select("bids_name","measure", "value") 

#select the most relevant measures
#selectionMeasure <- c("snr", "tsnr", "efc", "fber", "gsr_x", "gsr_y", "dvars_nstd", "dvars_std", "dvars_vstd", "gcor", "fd_mean", "fd_number", "fd_percentage", "spikes", "aor", "aqi")

#combine data
DF.full <- bind_rows(DF.dataset1, DF.dataset2, DF.dataset3,  DF.dataset4, .id = "dataset") %>%
  group_by(dataset) %>% 
  filter(measure == QCmeasure) #%in% c(selectionMeasure)) 

#create raincloud plot (check out the [github](https://github.com/RainCloudPlots/) or [preprint](https://wellcomeopenresearch.org/articles/4-63/v1)
p <- ggplot(DF.full,aes(x=dataset,y=value,fill=dataset))+
  geom_flat_violin(position=position_nudge(x = .2, y = 0),adjust =2, trim = FALSE, alpha = .5, colour = NA)+
  geom_point(aes(colour = dataset), position=position_jitter(width = .05), size = .5, shape = 20)+
  geom_boxplot(aes(x=dataset,y=value),position=position_nudge(x = .1, y = 0),outlier.shape = NA, alpha = .5, width = .1, colour = "black")+ 
  #facet_wrap(. ~ dataset) +
  theme_classic() + ylab(QCmeasure) + scale_fill_brewer(palette = "Reds") +
  scale_colour_brewer(palette = "Reds") + ggtitle(paste("Comparison of", modality, "QC measure", QCmeasure, "between datasets")) +
  facet_wrap(~measure)
p
```

```{r}
library(devtools)
library(tidystats)
#install_github("willemsleegers/tidystats")

#DF.full <- 
  
bind_rows(DF.dataset1, DF.dataset2, DF.dataset3,  DF.dataset4, .id = "dataset") %>%
  group_by(dataset, measure) %>%
  describe_data(value)
 # summarise(m = mean(value),
 #            sd = sd(value)) #%>%
  
  #write_csv("/Volumes/Project0255/code/MRIQC.csv")
  
```


## 3.3 fMRIprep 
fMRIprep is a docker tool for preprocessing of the fMRI data. More info [here](https://fmriprep.readthedocs.io/en/stable/#)

fMRIprep version 1.5.2 was used on a local iMac.

If you run into memory problems you can use --skip_bids_validation; skipped the --write-graph flag to save space.  --use-syn-sdc only for dataset_1 and datatset_2.

--use-syn-sdc for dataset_1 and dataset_2

If run on the GRID (HCP @ University of Glasgow), cd into the analyse folder and run:
```{bash}
singularity run --cleanenv /analyse/Project0255/my_images/fmriprep-1.5.2.simg /analyse/Project0255/dataset_1/ /analyse/Project0255/dataset_1/derivatives participant --participant-label sub-129 --fs-license-file /analyse/Project0255/my_images/license.txt --skip_bids_validation --use-syn-sd --fs-no-reconall -w /analyse/Project0255/work/compute00
```

Resize functional files for two participants (sub-117 and sub-125) from dataset_1 (sub-{sub}_ses-01_task-movie_space-MNI152NLin2009cAsym_desc-preproc_bold.nii) to allow for group comparison. 

```
voxsiz = [3 3 3.5]; % new voxel size {mm}
V = spm_select([1 Inf],'image');
V = spm_vol(V);
for i=1:numel(V)
   bb        = spm_get_bbox(V(i));
   VV(1:2)   = V(i);
   VV(1).mat = spm_matrix([bb(1,:) 0 0 0 voxsiz])*spm_matrix([-1 -1 -1]);
   VV(1).dim = ceil(VV(1).mat \ [bb(2,:) 1]' - 0.1)';
   VV(1).dim = VV(1).dim(1:3);
   spm_reslice(VV,struct('mean',false,'which',1,'interp',0)); % 1 for linear
end
```

## 3.4 Theory-of-Mind event protocols 

Create tsv file for functional localiser

Event coding (in s;  10s of fixation before movie starts; accounting for hemodynamic lag) is based on Richardson et al. 2018 - reverse correlation analyses. 
 
Note: For sub-322 the trigger was at the start of the movie (thus create a different tsv, with event - 10s).
Check the triggers for dataset_1.
```{r}
PartlyCloudy <- data.frame(onset = c(86, 98, 120, 176, 238, 252, 300, 70, 92, 106, 136, 194, 210, 228, 262, 312), #create the events (same for every sub)
                           duration = c(4, 6, 4, 16, 6, 8, 6, 4, 2, 4, 10, 4, 12, 6, 6, 4),
                           trial_type = c(rep("mental",7), rep("pain",9)))

#dataset_1
for (sub in 102:129){ #note: localisers for sub-101 are in ses-02
  filename = paste("/Volumes/Project0255/dataset_1/sub-", sub, "/ses-01/func/sub-", sub, "_ses-01_task-movie_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
#dataset_2
for (sub in 201:235){ 
  filename = paste("/Volumes/Project0255/dataset_2/sub-", sub, "/func/sub-", sub, "_task-movie_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
#dataset_3
for (sub in 301:322){ #note: localisers for sub-322 should have t-10 (no trigger) <-manually correct this
  filename = paste("/Volumes/Project0255/dataset_3/sub-", sub, "/func/sub-", sub, "_task-movie_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
#dataset_4
for (sub in 401:422){ 
  filename = paste("/Volumes/Project0255/dataset_4/sub-", sub, "/func/sub-", sub, "_task-movie_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
```

# 4. Analyses {.tabset}

## 4.1 First-level analysis 

Dataset_1 
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset1"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset1_ppn101"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset1_ppn114"
```

Dataset_2 
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset2_ppn201_202"
```

Dataset_3
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset3"
```

Dataset_4
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_firstlevel_tom_dataset4"
```

## 4.2 Second-level whole-brain analysis 

Create a group average for the GM_probseg.nii for each dataset in Matlab (change the code per dataset)
```{}
clear all

spm('Defaults','fMRI');
spm_jobman('initcfg');  

BIDS = spm_BIDS('/Volumes/Project0255/dataset_3'); %change this
BIDSfirst=fullfile(BIDS.dir,'derivatives/fmriprep'); 

sublist = transpose(BIDS.participants.participant_id) 
subex = [] %subjects that don't have an anatomical (14 dataset_1)
sublist(subex) = []; 

for i=1:length(sublist)
    subdir=fullfile(BIDSfirst,sublist{i}, 'anat')
    matlabbatch{1}.spm.util.imcalc.input{i,1} = [subdir, filesep, sublist{i}, '_space-MNI152NLin2009cAsym_label-GM_probseg.nii,1']
end
matlabbatch{1}.spm.util.imcalc.output = 'dataset3_averageGM';
matlabbatch{1}.spm.util.imcalc.outdir = {'/Volumes/Project0255/dataset_3/derivatives/fmriprep'}; %change this
matlabbatch{1}.spm.util.imcalc.expression = 'mean(X)';
matlabbatch{1}.spm.util.imcalc.var = struct('name', {}, 'value', {});
matlabbatch{1}.spm.util.imcalc.options.dmtx = 1;
matlabbatch{1}.spm.util.imcalc.options.mask = 0;
matlabbatch{1}.spm.util.imcalc.options.interp = 1;
matlabbatch{1}.spm.util.imcalc.options.dtype = 4;

spm_jobman('run',matlabbatch);
```

Run it seperately for the datasets.
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_secondlevel_tom_dataset1"
matlab -batch "BIDS_SPM_secondlevel_tom_dataset2"
matlab -batch "BIDS_SPM_secondlevel_tom_dataset3"
matlab -batch "BIDS_SPM_secondlevel_tom_dataset4"
```


## 4.3 ToM fROI analysis  

Run the following (ROI_extract.m) script in matlab (change the code per dataset and roi and contrast)
```{}
%========================================================================
%     fROI analysis for fmriprep data in BIDS format
%========================================================================
%     This script is written by  Michaela Kent and Ruud Hortensius
%     (University of Glasgow) 
%
%     Last updated: January 2020
%========================================================================
clear all
%add marsbar to path
marsbar('on')

%% Inputdirs
BIDS = spm_BIDS('/Volumes/Project0255/dataset_4'); % parse BIDS directory (easier to query info from dataset)
BIDSsecond=fullfile(BIDS.dir,'derivatives/bids_spm/second_level'); % get the second-level directory

contrastid = 'mental' %can be either mental (vs. pain) or pain (vs. mental)
networkid = 'tom' %can be either tom (theory-of-mind) or pain (pain matrix)

%% Outputdirs
outputdir=fullfile(BIDS.dir,'derivatives/roi', networkid);  % root outputdir for sublist
spm_mkdir(outputdir); % create output directory 

%% Load design matrix
spm_name = spm_load(fullfile(BIDSsecond, filesep, contrastid , 'SPM.mat'))
D  = mardo(spm_name);


%% Load rois
parcels = dir(fullfile(BIDS.dir,'derivatives/parcels/', networkid))
parcels = struct2cell(parcels(arrayfun(@(x) ~strcmp(x.name(1),'.'),parcels)))
parcels(2:6,:) = []

for i=1:length(parcels) 
    roi = fullfile(BIDS.dir,'derivatives/parcels/',  networkid, parcels{i})
    R  = maroi(roi);
    % Fetch data into marsbar data object
    mY  = get_marsy(R, D, 'mean');
    roi_data = summary_data(mY); % get summary time course(s)
    roi_name = [outputdir,filesep,parcels{i},'.tsv'];
    dlmwrite(roi_name,roi_data);
end
```

Add sub-201 and sub-202 (different parameters so now combined in whole-brain group analysis)
```{bash}
cd "/Volumes/Project0255/code/"
matlab -batch "BIDS_SPM_secondlevel_tom_dataset2_201_202"
matlab -batch "ROI_extract_201_202"

```

# 5. IDAQ {.tabset}

## 5.1 Calculation of individual scores

Dataset 2: sub-206-212, 219, 221-22, 224-25, 228, 231, 233-34 completed a version with 1-10 instead of 0-10
```{r}
sub_ex = c(206:212, 219, 221:222, 224:225, 228, 231, 233:234)
```


```{r}
library(tidyverse)

#load data
DF.d1  <- read_csv(file = paste("/Volumes/Project0255/dataset_1/sourcedata/IDAQ_dataset1.csv", sep ="")) %>%
  gather("sub", "value", 3:31)

DF.d2  <- read_csv(file = paste("/Volumes/Project0255/dataset_2/sourcedata/IDAQ_dataset2.csv", sep ="")) %>%
  gather("sub", "value", 3:37) 

DF.d3  <- read_csv(file = paste("/Volumes/Project0255/dataset_3/sourcedata/IDAQ_dataset3.csv", sep ="")) %>%
  gather("sub", "value", 3:24) 

DF.d4  <- read_csv(file = paste("/Volumes/Project0255/dataset_4/sourcedata/IDAQ_dataset4.csv", sep ="")) %>%
  gather("sub", "value", 3:24) 

DF.idaq <- bind_rows(DF.d1, DF.d2, DF.d3, DF.d4, .id = "dataset") %>%
  mutate(sub=gsub('sub-','',sub))%>%
  transform(sub=as.integer(sub)) %>%
  mutate(Subscale = as.factor(ifelse(Subscale == "IDAQ-NA", "IDAQNA", "IDAQ")))

rm(DF.d1, DF.d2, DF.d3, DF.d4)
```

## 5.2 Reliability of IDAQ

```{r}
library("psych")

#IDAQ
DF.idaq %>% 
  filter(Subscale == "IDAQ") %>%
  #filter(sub != sub_ex) %>% 
  spread(ItemNr, value) %>%
  select(-sub, -Subscale, -dataset) %>%
  alpha()


```

## 5.3 Reliability of IDAQ-NA

```{r}
#IDAQ-NA
DF.idaq %>% 
  filter(Subscale == "IDAQNA") %>%
  spread(ItemNr, value) %>%
  select(-sub, -Subscale, -dataset) %>%
  alpha(na.rm = TRUE)

```

## 5.4 IDAQ per subject

```{r}
DF.idaq <- DF.idaq %>%
  group_by(sub,dataset, Subscale) %>%
  summarise(score = sum(value, na.rm = TRUE)) 
```


## 5.5 Visualise the scores
```{r}
source("/Volumes/Project0255/code/R_rainclouds.R")


p <- DF.idaq %>%
 ggplot(.,aes(x=dataset,y=score,fill=dataset, group = dataset))+
  geom_flat_violin(position=position_nudge(x = .2, y = 0),adjust =2, trim = FALSE, alpha = .75, colour = NA)+
  geom_point(aes(colour = dataset), position=position_jitter(width = .05), size = .5, shape = 20)+
  geom_boxplot(aes(x=dataset,y=score),position=position_nudge(x = .1, y = 0),outlier.shape = NA, alpha = .5, width = .1, colour = "black")+ scale_fill_brewer(palette = "Blues") +
  scale_colour_brewer(palette = "Blues") +
  theme_classic() + ylab(paste("score (0-150)"))  + ggtitle(paste("Comparison of IDAQ scores between datasets")) +
  facet_wrap(~Subscale)

p
```

## 5.6 Median and interquartile range per dataset
```{r}
DF.idaq %>%
  group_by(dataset, Subscale) %>%
  summarise(median = median(score),
            iqr = IQR(score))
```

## 5.7 Comparison across datasets
```{r}
library("ez")
idaq_ez <- DF.idaq %>% ezANOVA(dv = score, wid = sub, within = .(Subscale), between = .(dataset))
print(idaq_ez)
```

# 6. fROI results {.tabset}

## 6.1 Data wrangling

Create function to load the data
```{r}
#load necessary libraries
library("tidyverse")
library("fs")

#create function to load roi datasets for the different networks
roi_extract <- function(datasetno, substart, subend, network, nroi) {
  
  dir_ls(paste("/Volumes/Project0255/dataset_", datasetno, "/derivatives/roi/", network, sep = ""), regexp = "\\.tsv$") %>% 
    map_dfr(read.delim, sep = "\t", .id = "id", header = FALSE)  %>%
    mutate(dataset = datasetno) %>%
    mutate(network = network) %>%
    mutate(network = str_extract(network, "tom|pain")) %>%
    mutate(id = str_extract(id, "dmpfc|mmpfc|vmpfc|ltpj|rtpj|prec|amcc|lmfg|rmfg|ls2|rs2|linsula|rinsula")) %>%
    rename(roi = id, contrast = V1) %>%
    mutate(sub = rep(substart:subend, times=nroi, each=1)) %>%
    select(5,3,4,1:2)
} 
```

Load the data for the Theory-of-Mind network
```{r}
DF.d1 <- roi_extract(1, 101, 129, "tom", 6)
DF.d2.a <- roi_extract(2, 201, 202, "tom/201_202", 6)
DF.d2.b <- roi_extract(2, 203, 235, "tom", 6)
DF.d3 <- roi_extract(3, 301, 322, "tom", 6)
DF.d4 <- roi_extract(4, 401, 422, "tom", 6)

DF.temp <- bind_rows(DF.d1, DF.d2.a, DF.d2.b, DF.d3, DF.d4) 

```

Load the data for the Pain Matrix network
```{r}
DF.d1 <- roi_extract(1, 101, 129, "pain", 7)
DF.d2.a <- roi_extract(2, 201, 202, "pain/201_202/", 7)
DF.d2.b <- roi_extract(2, 203, 235, "pain", 7)
DF.d3 <- roi_extract(3, 301, 322, "pain", 7)
DF.d4 <- roi_extract(4, 401, 422, "pain", 7)

DF.roi <- bind_rows(DF.temp, DF.d1, DF.d2.a, DF.d2.b, DF.d3, DF.d4)

rm(DF.d1, DF.d2.a, DF.d2.b, DF.d3, DF.d4, DF.temp)
```

## 6.2 Theory-of-Mind network activation across datasets

```{r}
source("/Volumes/Project0255/code/R_rainclouds.R")

p <- DF.roi %>%
  filter(network == "tom") %>%
  ggplot(.,aes(x=roi,y=contrast,fill=roi))+
  geom_flat_violin(position=position_nudge(x = .2, y = 0),adjust =2, trim = FALSE, alpha = .75, colour = NA)+
  geom_point(aes(colour = roi), position=position_jitter(width = .05), size = .5, shape = 20)+
  geom_boxplot(aes(x=roi,y=contrast),position=position_nudge(x = .1, y = 0),outlier.shape = NA, alpha = .5, width = .1, colour = "black")+ 
  theme_classic() + ylab(paste("contrast (mental vs pain)")) + scale_fill_brewer(palette = "Blues") +
  scale_colour_brewer(palette = "Blues") + ggtitle(paste("Comparison of ToM network contrasts values between datasets")) + theme(legend.position="none") +
  facet_wrap(~dataset)
p
```

## 6.3 Comparison across datasets - Theory-of-Mind network

```{r}
library("ez")
tom_ez <- DF.roi %>%
  filter(network == "tom") %>%
  ezANOVA(dv = contrast, wid = sub, within = .(roi), between = .(dataset))
print(tom_ez)

```
## 6.4 Pain Matrix activation across datasets

```{r}
source("/Volumes/Project0255/code/R_rainclouds.R")

p <- DF.roi %>%
  filter(network == "pain") %>%
  ggplot(.,aes(x=roi,y=contrast,fill=roi))+
  geom_flat_violin(position=position_nudge(x = .2, y = 0),adjust =2, trim = FALSE, alpha = .75, colour = NA)+
  geom_point(aes(colour = roi), position=position_jitter(width = .05), size = .5, shape = 20)+
  geom_boxplot(aes(x=roi,y=contrast),position=position_nudge(x = .1, y = 0),outlier.shape = NA, alpha = .5, width = .1, colour = "black")+ 
  theme_classic() + ylab(paste("contrast (mental vs pain)")) + scale_fill_brewer(palette = "Reds") +
  scale_colour_brewer(palette = "Reds") + ggtitle(paste("Comparison of ToM network contrasts values between datasets")) + theme(legend.position="none") +
  facet_wrap(~dataset)
p
```

## 6.5 Comparison across datasets - Pain Matrix

```{r}
library("ez")
pain_ez <- DF.roi %>%
  filter(network == "pain") %>%
  ezANOVA(dv = contrast, wid = sub, within = .(roi), between = .(dataset))
print(pain_ez)

```
# 7. IDAQ and ROI Analysis 

## 7.1 Combine IDAQ scores and ROI data
```{r}
DF.roi <- DF.idaq %>% ungroup() %>% mutate(dataset = as.numeric(dataset)) %>% left_join(DF.roi, DF.idaq, by = c("sub","dataset"), keep = FALSE) %>%
  spread(key=Subscale, value=score) #change this
```

#center the variables

```{r}
DF.roi$cent_IDAQNA <- scale(DF.roi$IDAQNA, scale = FALSE)
DF.roi$cent_IDAQ <- scale(DF.roi$IDAQ, scale = FALSE)

DF.roi <- DF.roi %>% group_by(roi) %>% mutate(cent_contrast = scale(contrast, scale =FALSE))
```

```{r}
x <- DF.roi$cent_IDAQ
y <- DF.roi$cent_contrast
plot (x, y,
      xlab = "log lex. frequency",
ylab = "RT(ms)",
abline(LF2, col="red", lwd=2))

# Calculate intercept (beta_0) and
# slope (beta_1) "on foot"
slope <- cor(x, y) * (sd(y) / sd(x)) 
intercept <- mean(y) - slope * mean(x)
# Look at what we’ve done
c("beta_0" = intercept, "beta_1" = slope)

LF1 <- lm(y ~ x) 
LF2 <- lm(y ~ x+ I(x^2))  
summary(LF1)

summary(LF2)

library(ordinal)
ordinal.mod <- clm(cent_IDAQ ~ cent_contrast, data = DF.roi) 
summary(ordinal.mod)

```

## 7.2 Correlations

```{r}
#want data in wide format
#make sure package GGally is installed 

library("GGally")

cormat <- DF.roi %>% 
  unite(network_roi, network, roi) %>%
  spread(network_roi, contrast)

cormat <- round(cor(as.matrix(cormat[3:20]), use = "complete.obs"), digits=3)

cormat

ggcorr(cormat, 
       geom="circle", 
       max_size=15,
       min_size=4,
       name="Correlation (Pearson)"
       ) 
```



## 7.3 Linear relationship

```{r}
plot1 <- DF.roi %>%
  filter(network == "tom") %>%
  ggplot(aes(x=cent_IDAQ, y=cent_contrast, colour=roi)) +
  geom_point(alpha=0.5,show.legend = FALSE) +
  geom_smooth(method="lm", se=FALSE, show.legend = FALSE) +
  labs(
    x="IDAQ score",
    y="Contrast"
  ) + theme_classic() +
  facet_grid(~roi)

plot1

linearmod1 <- lm(IDAQ ~ contrast, data = DF.roi)
summary(linearmod1)

plot4 <- DF.roi %>%
  filter(network == "tom") %>%
  ggplot(aes(x=cent_IDAQ, y=cent_contrast, colour=roi)) +
  geom_point(alpha=0.5,show.legend = FALSE) +
  geom_smooth(method="lm", formula =y~splines::bs(x,3), se=FALSE, show.legend = FALSE) +
  labs(
    x="IDAQ score",
    y="Contrast"
  ) + theme_classic() +
  facet_grid(~roi)

plot4

linearmod4 <- glm(cent_IDAQ ~ cent_contrast, data=DF.roi)
summary(linearmod4)
```

## 7.4 Non-linear relationship

```{r}
#load the data

plot2 <- DF.roi %>%
  ggplot(aes(x=IDAQ, y=contrast, colour=roi)) +
  geom_point(alpha=0.5,show.legend = FALSE) +
  geom_smooth(method="loess", se=FALSE, show.legend = FALSE) +
  labs(
    x="IDAQ score",
    y="Contrast"
  ) + theme_classic() +
  facet_grid(.~roi)

plot2


```




```{r}
plot3 <- DF.roi %>%
  filter(network == "tom") %>%
  #filter(!sub %in% c(sub_ex)) %>%
  ggplot(aes(x=cent_IDAQ, y=cent_contrast, colour=roi)) +
  geom_point(alpha=0.5,show.legend = FALSE) +
  geom_smooth(method="lm", formula=y ~ x+ I(x^2), se=TRUE, show.legend=FALSE) +
  labs(
    x="IDAQ score",
    y="Contrast"
  ) + theme_classic() +
  facet_grid(.~roi)
plot3

linearmod3 <- lm(formula = IDAQ ~ contrast + I(x^2), data = DF.roi)
summary(linearmod3)

```





