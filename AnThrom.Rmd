---
output:
  html_notebook:
    number_sections: no
    theme: default
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---
#**AnThroM**
**testing the relation between Theory-of-Mind network activation and dispositional anthropomorphism**  
by *Ruud Hortensius (University of Glasgow) - June 2019 - ...*

# 1. Details {.tabset}

## 1.1 Data 
  
 Dataset_1: Bangor Imaging Unit; EMBOTS; *n*=28; full dataset and publication: [Cross...Hortensius (2019) PTRB](https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0034)  
 Dataset_2: Centre for Cognitive NeuroImaging; SHAREDBOTS; *n*=35 (including 2 pilot scans) publication: Hortensius & Cross, in preparation
 Dataset_3: Centre for Cognitive NeuroImaging; Social_Gradient_1; *n*=22. Two *n*=10 and BOLDlight; *n*=12 (ongoing, including two pilot scans) (same parameters)
 Dataset_4: Centre for Cognitive NeuroImaging; ...   

## 1.2 Neuroimaging procedure
 
 All participants completed a Theory-of-Mind localiser ([Jacoby et al., 2016](https://www.sciencedirect.com/science/article/pii/S1053811915010472); [Richardson et al. 2018](https://www.nature.com/articles/s41467-018-03399-2)) and an anatomical scan either in the same session or in two seperate sessions. During the localiser participants passively viewed a short 5.6 min animated film ([Partly Cloudy](https://www.pixar.com/partly-cloudy#partly-cloudy-1)). This movie includes scenes depicting pain (e.g. an alligator biting the main character) and events that trigger mentalizing (e.g. the main character revealing its intention). At the end of each experiment participants completed the Individual Differences in Anthropomorphism Questionnaire (IDAQ) ([Waytz et al., 2010](https://journals.sagepub.com/doi/full/10.1177/1745691610369336)). 
 
- BOLD:
 Dataset_1: 3x3x3.5 voxels, 32 slices, repetition time = 2s, echo time = 30ms  
 Dataset_2: 3mm isotropic, 37 slices, TR = 2s, TE = 30ms  
 Dataset_3: 2mm isotropic, 68 slices, TR = 2s, TE = 27ms  
 
- T1W:
 Dataset_1: 1mm isotropic resolution, TR = 12ms, TE = 3.5ms  
 Dataset_2 - 3: 1mm isotropic resolution, TR = 2.3s, TE = 29.6ms (ADNI)  
 
## 1.3 To do
- get Dicoms, create BIDS dataset, use sub-101, sub-201, sub-301 etc...  
- remove Dicoms after BIDS dataset creation  
- Deface datqset  
- run MRIQC
- run fMRIprep  
- fROI  

# 2. BIDS dataset {.tabset}

## 2.1 Creating the BIDS dataset


## 2.2 Anonymize the data
Data from experiment 1 need to be defaced using [Pydeface](https://github.com/poldracklab/pydeface).

```{bash}
#!/bin/bash

set -e 
####For loop that defaces the MRI per subject and replaces the old MRI with the new defaced MRI
rootfolder=/Volumes/Project0246/dataset_1

for subj in 01 03 07 12 13 17 18 19 22 23 24 28; do
	echo "Defacing participant $subj"
for session in 01;do
pydeface ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
rm -f ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
mv ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w_defaced.nii ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii 
done
done

for subj in 02 04 05 06 08 09 10 11 15 16 20 21 25 26 27; do
	echo "Defacing participant $subj"
for session in 02;do
pydeface ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
rm -f ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
mv ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w_defaced.nii ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii 
done
done
```

## 2.2 Compressed the data (can be deleted)
Use gzip to compress the data. Careful not to leave any sensitive information in the file (use --no-name)

```{bash}
#!/bin/bash

set -e 
####For loop that defaces the MRI per subject and replaces the old MRI with the new defaced MRI
rootfolder=/Volumes/Project0246/dataset_2

for subj in 01 03 07 12 13 17 18 19 22 23 24 28; do
	echo "Zipping participant $subj"
for session in 01;do
gzip --no-name ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
done
done

for subj in 02 04 05 06 08 09 10 11 15 16 20 21 25 26 27; do
	echo "Zipping participant $subj"
for session in 02;do
gzip --no-name ${rootfolder}/sub-${subj}/ses-${session}/anat/sub-${subj}_ses-${session}_T1w.nii
done
done
```

## 2.3 BIDS validation

Use the BIDS-Validator to check if the dataset is BIDS compliant.

```{bash}
docker run -ti --rm -v /Volumes/Project0246/dataset_1:/data:ro bids/validator /data
```

Below can be removed
```{r}
library(AnalyzeFMRI)

a <- f.read.nifti.header("/Volumes/Project0245/data_exp1/sub-12/ses-01/func/sub-12_ses-01_task-tom_bold.nii")

a[["pixdim"]][5]
```


# 3. Theory-of-Mind event protocols {.tabset}

Create tsv file for functional localiser

Event coding (in s;  10s of fixation before movie starts; accounting for hemodynamic lag) is based on Richardson et al. 2018 - reverse correlation analyses. 
```{r}
PartlyCloudy <- data.frame(onset = c(86, 98, 120, 176, 238, 252, 300, 70, 92, 106, 136, 194, 210, 228, 262, 312), #create the events (same for every sub)
                           duration = c(4, 6, 4, 16, 6, 8, 6, 4, 2, 4, 10, 4, 12, 6, 6, 4),
                           trial_type = c(rep("mental",7), rep("pain",9)))

#dataset_1
for (sub in 3:28){ #note: localisers for sub-01 and sub-02 are in ses-02
  sub <- ifelse(sub < 10, paste0("0", sub), sub)
  filename = paste("/Volumes/Project0246/dataset_1/sub-", sub, "/ses-01/func/sub-", sub, "_ses-01_task-tom_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
#dataset_2
for (sub in 1:33){ #note: ppn22 ToM is in ses-00
  sub <- ifelse(sub < 10, paste0("0", sub), sub)
  filename = paste("/Volumes/Project0246/dataset_2/sub-", sub, "/ses-01/func/sub-", sub, "_ses-01_task-tom_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}
#dataset_3
for (sub in 1:10){ 
  sub <- ifelse(sub < 10, paste0("0", sub), sub)
  filename = paste("/Volumes/Project0246/dataset_3/sub-", sub, "/ses-01/func/sub-", sub, "_ses-01_task-tom_events.tsv", sep ="")
write.table(PartlyCloudy, file = filename, sep="\t", row.names = FALSE, quote = FALSE)
}

```

# 4. Preprocessing {.tabset}

## 4.1 MRIQC 
MRIQC is a docker tool to do quality control of the data. More info [here](https://poldracklab.github.io/mriqc/).

MRIQC 0.14.2 was used:

```{bash}
docker run -it --rm -v /Volumes/Project0246/dataset_1/:/data:ro -v /Volumes/Project0245/dataset_1/derivatives/mriqc:/out poldracklab/mriqc:0.14.2 /data /out participant --participant-label 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 -m T1w bold --ica --fft-spikes-detector
```

Run it seperately for the datasets. Change participant to group to create the group reports.

## 4.2 fMRIprep 
fMRIprep is a docker tool for preprocessing of the fMRI data. More info [here](https://fmriprep.readthedocs.io/en/stable/#)

fMRIprep version 1.3.2 was used on the grid.

If you run into memory problems you can use --skip_bids_validation; skipped the --write-graph flag to save space

If run on the lab iMac, with 50GB of docker memory, and with --fs-no-reconall (no segmentation), it takes about 20 min per participant.

```{bash}
/Users/soba/.local/bin/fmriprep-docker /Volumes/Project0246/dataset_1/ /Volumes/Project0246/dataset_1/derivatives participant --participant-label 01 02 03 04 05 06 07 08 09 10 --fs-license-file /Users/soba/license.txt --fs-no-reconall --use-syn-sdc
```

Singularity run:

```{bash}
singularity run --cleanenv /analyse/Project0246/my_images/fmriprep-1.3.2.simg /Volumes/Project0246/dataset_1/ //Volumes/Project0246/dataset_1/derivatives participant --participant-label sub-01 sub-02 sub-03 sub-04 sub-05 --fs-license-file /analyse/Project0246/my_images/license.txt --skip_bids_validation --fs-no-reconall --use-syn-sdc --use-aroma
```

Run it seperately for the datasets.

# 4. IDAQ {.tabset}

## Calculation of individual scores

# 5. Results {.tabset}

## Comparison across studies
